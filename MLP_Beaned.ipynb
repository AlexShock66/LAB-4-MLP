{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load, Split, and Balance (1.5 points total)\n",
    "##### **[.5 points]**\n",
    "* (1) Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so later in the rubric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      TractId    State          County  TotalPop   Men  Women  Hispanic  \\\n0  1001020100  Alabama  Autauga County      1845   899    946       2.4   \n1  1001020200  Alabama  Autauga County      2172  1167   1005       1.1   \n2  1001020300  Alabama  Autauga County      3385  1533   1852       8.0   \n3  1001020400  Alabama  Autauga County      4267  2001   2266       9.6   \n4  1001020500  Alabama  Autauga County      9965  5054   4911       0.9   \n\n   White  Black  Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  \\\n0   86.3    5.2     0.0  ...   0.5          0.0         2.1         24.5   \n1   41.6   54.5     0.0  ...   0.0          0.5         0.0         22.2   \n2   61.4   26.5     0.6  ...   1.0          0.8         1.5         23.1   \n3   80.3    7.1     0.5  ...   1.5          2.9         2.1         25.9   \n4   77.5   16.4     0.0  ...   0.8          0.3         0.7         21.0   \n\n   Employed  PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n0       881         74.2        21.2           4.5         0.0           4.6  \n1       852         75.9        15.0           9.0         0.0           3.4  \n2      1482         73.3        21.1           4.8         0.7           4.7  \n3      1849         75.8        19.7           4.5         0.0           6.1  \n4      4787         71.4        24.1           4.5         0.0           2.3  \n\n[5 rows x 37 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TractId</th>\n      <th>State</th>\n      <th>County</th>\n      <th>TotalPop</th>\n      <th>Men</th>\n      <th>Women</th>\n      <th>Hispanic</th>\n      <th>White</th>\n      <th>Black</th>\n      <th>Native</th>\n      <th>...</th>\n      <th>Walk</th>\n      <th>OtherTransp</th>\n      <th>WorkAtHome</th>\n      <th>MeanCommute</th>\n      <th>Employed</th>\n      <th>PrivateWork</th>\n      <th>PublicWork</th>\n      <th>SelfEmployed</th>\n      <th>FamilyWork</th>\n      <th>Unemployment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1001020100</td>\n      <td>Alabama</td>\n      <td>Autauga County</td>\n      <td>1845</td>\n      <td>899</td>\n      <td>946</td>\n      <td>2.4</td>\n      <td>86.3</td>\n      <td>5.2</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>2.1</td>\n      <td>24.5</td>\n      <td>881</td>\n      <td>74.2</td>\n      <td>21.2</td>\n      <td>4.5</td>\n      <td>0.0</td>\n      <td>4.6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001020200</td>\n      <td>Alabama</td>\n      <td>Autauga County</td>\n      <td>2172</td>\n      <td>1167</td>\n      <td>1005</td>\n      <td>1.1</td>\n      <td>41.6</td>\n      <td>54.5</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>22.2</td>\n      <td>852</td>\n      <td>75.9</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>3.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1001020300</td>\n      <td>Alabama</td>\n      <td>Autauga County</td>\n      <td>3385</td>\n      <td>1533</td>\n      <td>1852</td>\n      <td>8.0</td>\n      <td>61.4</td>\n      <td>26.5</td>\n      <td>0.6</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.8</td>\n      <td>1.5</td>\n      <td>23.1</td>\n      <td>1482</td>\n      <td>73.3</td>\n      <td>21.1</td>\n      <td>4.8</td>\n      <td>0.7</td>\n      <td>4.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1001020400</td>\n      <td>Alabama</td>\n      <td>Autauga County</td>\n      <td>4267</td>\n      <td>2001</td>\n      <td>2266</td>\n      <td>9.6</td>\n      <td>80.3</td>\n      <td>7.1</td>\n      <td>0.5</td>\n      <td>...</td>\n      <td>1.5</td>\n      <td>2.9</td>\n      <td>2.1</td>\n      <td>25.9</td>\n      <td>1849</td>\n      <td>75.8</td>\n      <td>19.7</td>\n      <td>4.5</td>\n      <td>0.0</td>\n      <td>6.1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1001020500</td>\n      <td>Alabama</td>\n      <td>Autauga County</td>\n      <td>9965</td>\n      <td>5054</td>\n      <td>4911</td>\n      <td>0.9</td>\n      <td>77.5</td>\n      <td>16.4</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.8</td>\n      <td>0.3</td>\n      <td>0.7</td>\n      <td>21.0</td>\n      <td>4787</td>\n      <td>71.4</td>\n      <td>24.1</td>\n      <td>4.5</td>\n      <td>0.0</td>\n      <td>2.3</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 37 columns</p>\n</div>"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the data\n",
    "data = pd.read_csv('./acs2017_census_tract_data.csv', low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2) Remove any observations that having missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values before drop:\n",
      "\n",
      "TractId                0\n",
      "State                  0\n",
      "County                 0\n",
      "TotalPop               0\n",
      "Men                    0\n",
      "Women                  0\n",
      "Hispanic             696\n",
      "White                696\n",
      "Black                696\n",
      "Native               696\n",
      "Asian                696\n",
      "Pacific              696\n",
      "VotingAgeCitizen       0\n",
      "Income              1116\n",
      "IncomeErr           1116\n",
      "IncomePerCap         745\n",
      "IncomePerCapErr      745\n",
      "Poverty              842\n",
      "ChildPoverty        1110\n",
      "Professional         811\n",
      "Service              811\n",
      "Office               811\n",
      "Construction         811\n",
      "Production           811\n",
      "Drive                801\n",
      "Carpool              801\n",
      "Transit              801\n",
      "Walk                 801\n",
      "OtherTransp          801\n",
      "WorkAtHome           801\n",
      "MeanCommute          946\n",
      "Employed               0\n",
      "PrivateWork          811\n",
      "PublicWork           811\n",
      "SelfEmployed         811\n",
      "FamilyWork           811\n",
      "Unemployment         810\n",
      "dtype: int64\n",
      "\n",
      "Null Values after drop:\n",
      "\n",
      "TractId             0\n",
      "State               0\n",
      "County              0\n",
      "TotalPop            0\n",
      "Men                 0\n",
      "Women               0\n",
      "Hispanic            0\n",
      "White               0\n",
      "Black               0\n",
      "Native              0\n",
      "Asian               0\n",
      "Pacific             0\n",
      "VotingAgeCitizen    0\n",
      "Income              0\n",
      "IncomeErr           0\n",
      "IncomePerCap        0\n",
      "IncomePerCapErr     0\n",
      "Poverty             0\n",
      "ChildPoverty        0\n",
      "Professional        0\n",
      "Service             0\n",
      "Office              0\n",
      "Construction        0\n",
      "Production          0\n",
      "Drive               0\n",
      "Carpool             0\n",
      "Transit             0\n",
      "Walk                0\n",
      "OtherTransp         0\n",
      "WorkAtHome          0\n",
      "MeanCommute         0\n",
      "Employed            0\n",
      "PrivateWork         0\n",
      "PublicWork          0\n",
      "SelfEmployed        0\n",
      "FamilyWork          0\n",
      "Unemployment        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# data.describe()\n",
    "#Finding null values\n",
    "print(\"Null Values before drop:\\n\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "data.dropna(axis=0,how=\"any\",inplace=True)\n",
    "\n",
    "print(\"\\nNull Values after drop:\\n\")\n",
    "print(data.isnull().sum())\n",
    "# data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (3) Encode any string data as integers for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Features\n",
      "State     object\n",
      "County    object\n",
      "dtype: object\n",
      "Features After encoding\n",
      "State      int8\n",
      "County    int16\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorical Features\")\n",
    "print(data.dtypes[data.dtypes != 'float64'][data.dtypes !='int64'])\n",
    "data_cat = data.copy() # Will be used later\n",
    "states = data[\"State\"]\n",
    "counties = data[\"County\"]\n",
    "data[\"State\"] = data[\"State\"].astype('category')\n",
    "data[\"State\"] = data[\"State\"].cat.codes\n",
    "data[\"County\"] = data[\"County\"].astype('category')\n",
    "data[\"County\"] = data[\"County\"].cat.codes\n",
    "\n",
    "print(\"Features After encoding\")\n",
    "print(data.dtypes[data.dtypes != 'float64'][data.dtypes !='int64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (4) You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We decided to keep the county data in the dataset instead of trying to take the mean across each state because we determined that there is so much variation between counties that trying to classify them at a state level would lead to a much lower accuracy.<br/>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We chose to remove variables such as TractId because it is an ID number and not relevant as a predictor of our classes. We also removed things such as race because we wanted to try to stray away from the model picking up a racial bias and wanted it to focus more on variables such as income and the types of industry in the given county.<br/>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We also removed data such as how people commute to work as we determined that this data would not be very important in predicting child poverty as the types of commuting vary drastically and would only really help to determine if a county has most of the population in a city or not and it is not a good indicator as to whether or not a county will have high child poverty. It was also necessary to convert data such as number of Men and number of Women to be percentages so that they were somewhat normalized because without doing this, then a county with a higher population will appear to have more women and men than another county with lower pop. Finally, we removed data about the number of people over the age of 16 who are employed because it is already accounted for in the unemployment rate attribute and by leaving it in, we would create a stronger bias for that data without meaning to.<br/>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__NOTE:__ It may be worth while to add this data in back in the end to see if it was increases our accuracy but for now, we will leave it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              State        County      TotalPop           Men         Women  \\\ncount  72718.000000  72718.000000  72718.000000  72718.000000  72718.000000   \nmean      24.340370    998.356941   4443.485121      0.491322      0.508678   \nstd       15.102552    530.254496   2190.183318      0.040244      0.040244   \nmin        0.000000      0.000000     58.000000      0.037275      0.006886   \n25%       10.000000    545.000000   2958.000000      0.469348      0.488619   \n50%       24.000000   1045.000000   4137.000000      0.490761      0.509239   \n75%       38.000000   1433.750000   5532.750000      0.511381      0.530652   \nmax       51.000000   1953.000000  65528.000000      0.993114      0.962725   \n\n       VotingAgeCitizen         Income      IncomeErr   IncomePerCap  \\\ncount      72718.000000   72718.000000   72718.000000   72718.000000   \nmean           0.717249   61119.999326    9690.325642   30666.653222   \nstd            0.104256   30511.062580    6119.407315   15844.127467   \nmin            0.083826    2692.000000     728.000000    1631.000000   \n25%            0.672307   40380.000000    5737.000000   20624.000000   \n50%            0.739374   54413.000000    8268.000000   27249.000000   \n75%            0.783471   74688.000000   11909.000000   36413.000000   \nmax            0.992776  249750.000000  153365.000000  220253.000000   \n\n       IncomePerCapErr  ...       Service        Office  Construction  \\\ncount     72718.000000  ...  72718.000000  72718.000000  72718.000000   \nmean       4249.725969  ...     18.847948     23.413165      9.263044   \nstd        2991.009809  ...      7.969609      5.591354      5.943849   \nmin         351.000000  ...      0.000000      0.000000      0.000000   \n25%        2508.000000  ...     13.300000     19.700000      5.000000   \n50%        3404.000000  ...     17.700000     23.200000      8.400000   \n75%        4959.000000  ...     23.200000     26.900000     12.500000   \nmax       84414.000000  ...     70.900000     72.300000     68.100000   \n\n         Production    WorkAtHome   PrivateWork    PublicWork  SelfEmployed  \\\ncount  72718.000000  72718.000000  72718.000000  72718.000000  72718.000000   \nmean      12.922312      4.612646     79.511827     14.149495      6.167661   \nstd        7.592511      3.770733      7.957350      7.164790      3.798703   \nmin        0.000000      0.000000     17.500000      0.000000      0.000000   \n25%        7.200000      2.000000     75.300000      9.300000      3.500000   \n50%       11.800000      3.800000     80.600000     13.000000      5.500000   \n75%       17.500000      6.300000     85.000000     17.600000      8.000000   \nmax       60.500000     82.800000    100.000000     80.700000     47.400000   \n\n         FamilyWork  Unemployment  \ncount  72718.000000  72718.000000  \nmean       0.171231      7.224917  \nstd        0.451630      5.099419  \nmin        0.000000      0.000000  \n25%        0.000000      3.900000  \n50%        0.000000      6.000000  \n75%        0.000000      9.000000  \nmax       22.300000     62.800000  \n\n[8 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>State</th>\n      <th>County</th>\n      <th>TotalPop</th>\n      <th>Men</th>\n      <th>Women</th>\n      <th>VotingAgeCitizen</th>\n      <th>Income</th>\n      <th>IncomeErr</th>\n      <th>IncomePerCap</th>\n      <th>IncomePerCapErr</th>\n      <th>...</th>\n      <th>Service</th>\n      <th>Office</th>\n      <th>Construction</th>\n      <th>Production</th>\n      <th>WorkAtHome</th>\n      <th>PrivateWork</th>\n      <th>PublicWork</th>\n      <th>SelfEmployed</th>\n      <th>FamilyWork</th>\n      <th>Unemployment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>...</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n      <td>72718.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>24.340370</td>\n      <td>998.356941</td>\n      <td>4443.485121</td>\n      <td>0.491322</td>\n      <td>0.508678</td>\n      <td>0.717249</td>\n      <td>61119.999326</td>\n      <td>9690.325642</td>\n      <td>30666.653222</td>\n      <td>4249.725969</td>\n      <td>...</td>\n      <td>18.847948</td>\n      <td>23.413165</td>\n      <td>9.263044</td>\n      <td>12.922312</td>\n      <td>4.612646</td>\n      <td>79.511827</td>\n      <td>14.149495</td>\n      <td>6.167661</td>\n      <td>0.171231</td>\n      <td>7.224917</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>15.102552</td>\n      <td>530.254496</td>\n      <td>2190.183318</td>\n      <td>0.040244</td>\n      <td>0.040244</td>\n      <td>0.104256</td>\n      <td>30511.062580</td>\n      <td>6119.407315</td>\n      <td>15844.127467</td>\n      <td>2991.009809</td>\n      <td>...</td>\n      <td>7.969609</td>\n      <td>5.591354</td>\n      <td>5.943849</td>\n      <td>7.592511</td>\n      <td>3.770733</td>\n      <td>7.957350</td>\n      <td>7.164790</td>\n      <td>3.798703</td>\n      <td>0.451630</td>\n      <td>5.099419</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>58.000000</td>\n      <td>0.037275</td>\n      <td>0.006886</td>\n      <td>0.083826</td>\n      <td>2692.000000</td>\n      <td>728.000000</td>\n      <td>1631.000000</td>\n      <td>351.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>17.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>10.000000</td>\n      <td>545.000000</td>\n      <td>2958.000000</td>\n      <td>0.469348</td>\n      <td>0.488619</td>\n      <td>0.672307</td>\n      <td>40380.000000</td>\n      <td>5737.000000</td>\n      <td>20624.000000</td>\n      <td>2508.000000</td>\n      <td>...</td>\n      <td>13.300000</td>\n      <td>19.700000</td>\n      <td>5.000000</td>\n      <td>7.200000</td>\n      <td>2.000000</td>\n      <td>75.300000</td>\n      <td>9.300000</td>\n      <td>3.500000</td>\n      <td>0.000000</td>\n      <td>3.900000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>24.000000</td>\n      <td>1045.000000</td>\n      <td>4137.000000</td>\n      <td>0.490761</td>\n      <td>0.509239</td>\n      <td>0.739374</td>\n      <td>54413.000000</td>\n      <td>8268.000000</td>\n      <td>27249.000000</td>\n      <td>3404.000000</td>\n      <td>...</td>\n      <td>17.700000</td>\n      <td>23.200000</td>\n      <td>8.400000</td>\n      <td>11.800000</td>\n      <td>3.800000</td>\n      <td>80.600000</td>\n      <td>13.000000</td>\n      <td>5.500000</td>\n      <td>0.000000</td>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>38.000000</td>\n      <td>1433.750000</td>\n      <td>5532.750000</td>\n      <td>0.511381</td>\n      <td>0.530652</td>\n      <td>0.783471</td>\n      <td>74688.000000</td>\n      <td>11909.000000</td>\n      <td>36413.000000</td>\n      <td>4959.000000</td>\n      <td>...</td>\n      <td>23.200000</td>\n      <td>26.900000</td>\n      <td>12.500000</td>\n      <td>17.500000</td>\n      <td>6.300000</td>\n      <td>85.000000</td>\n      <td>17.600000</td>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>9.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>51.000000</td>\n      <td>1953.000000</td>\n      <td>65528.000000</td>\n      <td>0.993114</td>\n      <td>0.962725</td>\n      <td>0.992776</td>\n      <td>249750.000000</td>\n      <td>153365.000000</td>\n      <td>220253.000000</td>\n      <td>84414.000000</td>\n      <td>...</td>\n      <td>70.900000</td>\n      <td>72.300000</td>\n      <td>68.100000</td>\n      <td>60.500000</td>\n      <td>82.800000</td>\n      <td>100.000000</td>\n      <td>80.700000</td>\n      <td>47.400000</td>\n      <td>22.300000</td>\n      <td>62.800000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Cleaning\n",
    "#Drop Non important columns\n",
    "data.drop(columns=['TractId','Hispanic','White','Black','Native','Asian','Pacific','Employed','MeanCommute','OtherTransp','Walk','Transit','Carpool','Drive'],inplace=True)\n",
    "\n",
    "\n",
    "#Numerical Data into percentages so that it is not skewed by population\n",
    "data['Men'] = data['Men'] / data['TotalPop']\n",
    "data['Women'] = data['Women'] / data['TotalPop']\n",
    "data['VotingAgeCitizen'] = data['VotingAgeCitizen'] / data['TotalPop']\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining the cutoff for our Categories of child poverty\n",
    "##### [.5 points] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. Should balancing of the dataset be done for both the training and testing set? Explain.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We decided to go with pandas built in qcut function which is able to evenly divide any given series into n bins. Since we want 4 types of targets, we gave the function the number 4 for n and it ends up being the cutoff for the quartile ranges of the data. Meaning that the first bin, low child poverty, is the values from the 0th to 25th quartile and the moderate variable is the values from the 25th quartile to the 50th so on and so forth. By doing this, we are left with a roughly equivalent number of entries in each target variable. This is extremely important for the training data because without it, the model could only be good at looking at low poverty rates and would have little data to go off of in the high or extreme categories for example. This idea is not so important for the testing data because when testing, the model should be able to generalize such that it does not need an equivalent number of each class as it is not actively learning from the data and would therefore not be skewed one way or the other. I would argue that it is almost beneficial to have an uneven number of each category in the testing set because that can show if the model is able to generalize well or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChildPoverty\n",
      "1    18229\n",
      "2    18171\n",
      "3    18148\n",
      "4    18170\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Coming up with divisors for child poverty\n",
    "\n",
    "tmp = pd.qcut(data['ChildPoverty'],4,labels=[1, 2, 3, 4])\n",
    "# tmp = pd.qcut(data['ChildPoverty'],4,labels=['low','moderate','high','extreme'])\n",
    "data['ChildPoverty'] = tmp\n",
    "print(data.groupby(['ChildPoverty']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['State', 'County', 'TotalPop', 'Men', 'Women', 'VotingAgeCitizen',\n       'Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty',\n       'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction',\n       'Production', 'WorkAtHome', 'PrivateWork', 'PublicWork', 'SelfEmployed',\n       'FamilyWork', 'Unemployment'],\n      dtype='object')"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [.5 points] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing. There is NO NEED to split the data multiple times for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data shape: (58174, 23) - 80.00% of original data\n",
      "Train_data classes:\n",
      " ChildPoverty\n",
      "1    14596\n",
      "2    14447\n",
      "3    14538\n",
      "4    14593\n",
      "dtype: int64\n",
      "\n",
      "Test_data shape: (14544, 23) - 20.00% of original data\n",
      "Test_data classes:\n",
      " ChildPoverty\n",
      "1    3633\n",
      "2    3724\n",
      "3    3610\n",
      "4    3577\n",
      "dtype: int64\n",
      "\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Type of y: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "X_train: (58174, 22)\n",
      "y_train: (58174, 4)\n",
      "Targets: [0 1]\n",
      "X_test: (14544, 22)\n",
      "y_test: (14544, 4)\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encodes\n",
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    res = res.drop([feature_to_encode], axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Train_data shape: {train_data.shape} - {train_data.shape[0] / data.shape[0] * 100:.2f}% of original data')\n",
    "print('Train_data classes:\\n',train_data.groupby(['ChildPoverty']).size()) #Ensure that the number of classes stays relatively equivalent\n",
    "print(f'\\nTest_data shape: {test_data.shape} - {test_data.shape[0] / data.shape[0] * 100:.2f}% of original data')\n",
    "print('Test_data classes:\\n',test_data.groupby(['ChildPoverty']).size())\n",
    "\n",
    "y_train = pd.DataFrame(train_data['ChildPoverty'])\n",
    "# y_train = y_train.values.ravel()\n",
    "X_train = train_data.drop(columns=['ChildPoverty'],inplace=False)\n",
    "y_train = encode_and_bind(y_train,'ChildPoverty')\n",
    "\n",
    "y_test = pd.DataFrame(test_data['ChildPoverty'])\n",
    "# y_test = y_test.values.ravel()\n",
    "X_test = test_data.drop(columns=['ChildPoverty'],inplace=False)\n",
    "y_test = encode_and_bind(y_test,'ChildPoverty')\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Type of X: {type(X_train)}\\nType of y: {type(y_train)}\\n')\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('Targets:', np.unique(y_train))\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pre-processing and Initial Modeling (2.5 points total)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We will be using a two layer perceptron for the next few parts. The perceptron uses the following properties:\n",
    "1. Vectorized Gradient Computation - DONE\n",
    "2. Mini-Batching - TODO\n",
    "3. Cross Entropy Loss - TODO\n",
    "4. Proper Glorot Initialization - TODO\n",
    "5. Sigmoids - DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two Layer Perceptron Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,C=0.0,epochs=500,eta=0.001,random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        return pd.get_dummies(y).values.T\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        W1_num_elems = (self.n_features_ + 1) * self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0,1.0,size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden,self.n_features_ + 1)\n",
    "\n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0,1.0,size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_,self.n_hidden + 1)\n",
    "        return W1,W2\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs\n",
    "\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # need to vectorize this computation!\n",
    "        # See additional code and derivation below!\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        print(f\"========== A3.shapoe {A3.shape}\")\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2)\n",
    "\n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * grad1\n",
    "            self.W2 -= self.eta * grad2\n",
    "\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "\n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "params = dict(n_hidden=50,\n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=25, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nn = TwoLayerPerceptronVectorized(**params)\n",
    "\n",
    "nn.fit(X_train, y_train.T, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "\n",
    "print(np.unique(yhat))\n",
    "print(yhat.shape)\n",
    "print(y_train.shape)\n",
    "print('Accuracy:',accuracy_score(np.argmax(y_test,axis=1),yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58174,)\n"
     ]
    }
   ],
   "source": [
    "y_train.head()\n",
    "y_train_arr = np.array(y_train)\n",
    "\n",
    "y_train_arr.shape\n",
    "tmp = np.argmax(y_train_arr,axis= 1)\n",
    "print(tmp.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper Glorot Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you update the objective function, you must also update the get gradient function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **[.5 points]** Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **[.5 points]** Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **[.5 points]** Now (1) normalize the continuous numeric feature data AND (2) one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Because we need to hot encode the categorical data, we need to go back to the point before we convert the categorical data into numerical. Now that we have the old data, we can go through the pre-processing again and normalize the numerical features and one-hot encode the categorical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **[1 points]** Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The normalized model has a slightly better performance (25.6%) compared to the raw data model (24.8%), because the attributes with higher values have the same weight as the ones with lower values now that they are in the same scale. The improvement is surprinsingly low, and we expected the normalized model to do at least 1.5x better than normal data. The normalized and one-hot encoded model is --- than the other two. This is because machine language models, such as the two layer perceptron implemented above, have a much better time dealing with one-hot encoded attributes than integer encoding as it previously was. However, the one-hot encoding brought the curse of dimensionality, and because of the amount of states and counties, the number of attributes increased to 1500+ and thus the run-time also increased dramastically (about ten-fold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Modeling (5 points total)\n",
    "##### **[1 points]** Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation). For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm.\n",
    "Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **[1 points]** Repeat the previous step, adding support for a fourth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **[1 points]** Repeat the previous step, adding support for a fifth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **[2 points]** Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network (such as AdaGrad, RMSProps, or AdaDelta). Discuss which adaptive method you chose. Compare the performance of your five layer model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique as it is part of the exceptional work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exceptional Work (1 points total)\n",
    "5000 level student: You have free reign to provide additional analyses.\n",
    "One idea (required for 7000 level students):  Implement adaptive momentum (AdaM) in the five layer neural network and quantify the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "-0.5 0.5\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "X_train: (1437, 64)\n",
      "y_train: (1437,)\n",
      "X_test: (360, 64)\n",
      "y_test: (360,)\n"
     ]
    }
   ],
   "source": [
    "# Data from the notebook just to check that the functions are working and we have the same types for the X and y datasets\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "ds = load_digits()\n",
    "X = ds.data/16.0-0.5 # normalize the input, very important\n",
    "y = ds.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.min(X),np.max(X))\n",
    "print(np.unique(y))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_num, X_test_num, y_train_num, y_test_num = train_test_split(\n",
    "    X,y,test_size = 0.2)\n",
    "\n",
    "print('X_train:', X_train_num.shape)\n",
    "print('y_train:', y_train_num.shape)\n",
    "print('X_test:', X_test_num.shape)\n",
    "print('y_test:', y_test_num.shape)\n",
    "\n",
    "# nn.fit(X_train_num, y_train_num, print_progress=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "86969af4118fd2beaee010b14a97d3c8fe7dc31ff55f1528eea7eaecb45368d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
