{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load, Split, and Balance (1.5 points total)\n",
    "##### **[.5 points]**\n",
    "* (1) Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so later in the rubric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TractId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001020100</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1845</td>\n",
       "      <td>899</td>\n",
       "      <td>946</td>\n",
       "      <td>2.4</td>\n",
       "      <td>86.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>881</td>\n",
       "      <td>74.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001020200</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>2172</td>\n",
       "      <td>1167</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>54.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>852</td>\n",
       "      <td>75.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001020300</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>3385</td>\n",
       "      <td>1533</td>\n",
       "      <td>1852</td>\n",
       "      <td>8.0</td>\n",
       "      <td>61.4</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>23.1</td>\n",
       "      <td>1482</td>\n",
       "      <td>73.3</td>\n",
       "      <td>21.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001020400</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>4267</td>\n",
       "      <td>2001</td>\n",
       "      <td>2266</td>\n",
       "      <td>9.6</td>\n",
       "      <td>80.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>25.9</td>\n",
       "      <td>1849</td>\n",
       "      <td>75.8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001020500</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>9965</td>\n",
       "      <td>5054</td>\n",
       "      <td>4911</td>\n",
       "      <td>0.9</td>\n",
       "      <td>77.5</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4787</td>\n",
       "      <td>71.4</td>\n",
       "      <td>24.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TractId    State          County  TotalPop   Men  Women  Hispanic  \\\n",
       "0  1001020100  Alabama  Autauga County      1845   899    946       2.4   \n",
       "1  1001020200  Alabama  Autauga County      2172  1167   1005       1.1   \n",
       "2  1001020300  Alabama  Autauga County      3385  1533   1852       8.0   \n",
       "3  1001020400  Alabama  Autauga County      4267  2001   2266       9.6   \n",
       "4  1001020500  Alabama  Autauga County      9965  5054   4911       0.9   \n",
       "\n",
       "   White  Black  Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  \\\n",
       "0   86.3    5.2     0.0  ...   0.5          0.0         2.1         24.5   \n",
       "1   41.6   54.5     0.0  ...   0.0          0.5         0.0         22.2   \n",
       "2   61.4   26.5     0.6  ...   1.0          0.8         1.5         23.1   \n",
       "3   80.3    7.1     0.5  ...   1.5          2.9         2.1         25.9   \n",
       "4   77.5   16.4     0.0  ...   0.8          0.3         0.7         21.0   \n",
       "\n",
       "   Employed  PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0       881         74.2        21.2           4.5         0.0           4.6  \n",
       "1       852         75.9        15.0           9.0         0.0           3.4  \n",
       "2      1482         73.3        21.1           4.8         0.7           4.7  \n",
       "3      1849         75.8        19.7           4.5         0.0           6.1  \n",
       "4      4787         71.4        24.1           4.5         0.0           2.3  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the data\n",
    "data = pd.read_csv('./acs2017_census_tract_data.csv', low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2) Remove any observations that having missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values before drop:\n",
      "\n",
      "TractId             0\n",
      "State               0\n",
      "County              0\n",
      "TotalPop            0\n",
      "Men                 0\n",
      "Women               0\n",
      "Hispanic            0\n",
      "White               0\n",
      "Black               0\n",
      "Native              0\n",
      "Asian               0\n",
      "Pacific             0\n",
      "VotingAgeCitizen    0\n",
      "Income              0\n",
      "IncomeErr           0\n",
      "IncomePerCap        0\n",
      "IncomePerCapErr     0\n",
      "Poverty             0\n",
      "ChildPoverty        0\n",
      "Professional        0\n",
      "Service             0\n",
      "Office              0\n",
      "Construction        0\n",
      "Production          0\n",
      "Drive               0\n",
      "Carpool             0\n",
      "Transit             0\n",
      "Walk                0\n",
      "OtherTransp         0\n",
      "WorkAtHome          0\n",
      "MeanCommute         0\n",
      "Employed            0\n",
      "PrivateWork         0\n",
      "PublicWork          0\n",
      "SelfEmployed        0\n",
      "FamilyWork          0\n",
      "Unemployment        0\n",
      "dtype: int64\n",
      "\n",
      "Null Values after drop:\n",
      "\n",
      "TractId             0\n",
      "State               0\n",
      "County              0\n",
      "TotalPop            0\n",
      "Men                 0\n",
      "Women               0\n",
      "Hispanic            0\n",
      "White               0\n",
      "Black               0\n",
      "Native              0\n",
      "Asian               0\n",
      "Pacific             0\n",
      "VotingAgeCitizen    0\n",
      "Income              0\n",
      "IncomeErr           0\n",
      "IncomePerCap        0\n",
      "IncomePerCapErr     0\n",
      "Poverty             0\n",
      "ChildPoverty        0\n",
      "Professional        0\n",
      "Service             0\n",
      "Office              0\n",
      "Construction        0\n",
      "Production          0\n",
      "Drive               0\n",
      "Carpool             0\n",
      "Transit             0\n",
      "Walk                0\n",
      "OtherTransp         0\n",
      "WorkAtHome          0\n",
      "MeanCommute         0\n",
      "Employed            0\n",
      "PrivateWork         0\n",
      "PublicWork          0\n",
      "SelfEmployed        0\n",
      "FamilyWork          0\n",
      "Unemployment        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# data.describe()\n",
    "#Finding null values\n",
    "print(\"Null Values before drop:\\n\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "data.dropna(axis=0,how=\"any\",inplace=True)\n",
    "\n",
    "print(\"\\nNull Values after drop:\\n\")\n",
    "print(data.isnull().sum())\n",
    "# data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (3) Encode any string data as integers for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Features\n",
      "State     object\n",
      "County    object\n",
      "dtype: object\n",
      "Features After encoding\n",
      "State      int8\n",
      "County    int16\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorical Features\")\n",
    "print(data.dtypes[data.dtypes != 'float64'][data.dtypes !='int64'])\n",
    "data[\"State\"] = data[\"State\"].astype('category')\n",
    "data[\"State\"] = data[\"State\"].cat.codes\n",
    "data[\"County\"] = data[\"County\"].astype('category')\n",
    "data[\"County\"] = data[\"County\"].cat.codes\n",
    "\n",
    "print(\"Features After encoding\")\n",
    "print(data.dtypes[data.dtypes != 'float64'][data.dtypes !='int64'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (4) You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We decided to keep the county data in the dataset instead of trying to take the mean across each state because we determined that there is so much variation between counties that trying to classify them at a state level would lead to a much lower accuracy.<br/>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We chose to remove variables such as TractId because it is an ID number and not relevant as a predictor of our classes. We also removed things such as race because we wanted to try to stray away from the model picking up a racial bias and wanted it to focus more on variables such as income and the types of industry in the given county.<br/>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We also removed data such as how people commute to work as we determined that this data would not be very important in predicting child poverty as the types of commuting vary drastically and would only really help to determine if a county has most of the population in a city or not and it is not a good indicator as to whether or not a county will have high child poverty. It was also necessary to convert data such as number of Men and number of Women to be percentages so that they were somewhat normalized because without doing this, then a county with a higher population will appear to have more women and men than another county with lower pop. Finally, we removed data about the number of people over the age of 16 who are employed because it is already accounted for in the unemployment rate attribute and by leaving it in, we would create a stronger bias for that data without meaning to.<br/>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__NOTE:__ It may be worth while to add this data in back in the end to see if it was increases our accuracy but for now, we will leave it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>VotingAgeCitizen</th>\n",
       "      <th>Income</th>\n",
       "      <th>IncomeErr</th>\n",
       "      <th>IncomePerCap</th>\n",
       "      <th>IncomePerCapErr</th>\n",
       "      <th>...</th>\n",
       "      <th>Service</th>\n",
       "      <th>Office</th>\n",
       "      <th>Construction</th>\n",
       "      <th>Production</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "      <td>72718.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.340370</td>\n",
       "      <td>998.356941</td>\n",
       "      <td>4443.485121</td>\n",
       "      <td>0.491322</td>\n",
       "      <td>0.508678</td>\n",
       "      <td>0.717249</td>\n",
       "      <td>61119.999326</td>\n",
       "      <td>9690.325642</td>\n",
       "      <td>30666.653222</td>\n",
       "      <td>4249.725969</td>\n",
       "      <td>...</td>\n",
       "      <td>18.847948</td>\n",
       "      <td>23.413165</td>\n",
       "      <td>9.263044</td>\n",
       "      <td>12.922312</td>\n",
       "      <td>4.612646</td>\n",
       "      <td>79.511827</td>\n",
       "      <td>14.149495</td>\n",
       "      <td>6.167661</td>\n",
       "      <td>0.171231</td>\n",
       "      <td>7.224917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.102552</td>\n",
       "      <td>530.254496</td>\n",
       "      <td>2190.183318</td>\n",
       "      <td>0.040244</td>\n",
       "      <td>0.040244</td>\n",
       "      <td>0.104256</td>\n",
       "      <td>30511.062580</td>\n",
       "      <td>6119.407315</td>\n",
       "      <td>15844.127467</td>\n",
       "      <td>2991.009809</td>\n",
       "      <td>...</td>\n",
       "      <td>7.969609</td>\n",
       "      <td>5.591354</td>\n",
       "      <td>5.943849</td>\n",
       "      <td>7.592511</td>\n",
       "      <td>3.770733</td>\n",
       "      <td>7.957350</td>\n",
       "      <td>7.164790</td>\n",
       "      <td>3.798703</td>\n",
       "      <td>0.451630</td>\n",
       "      <td>5.099419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.037275</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>0.083826</td>\n",
       "      <td>2692.000000</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>1631.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>2958.000000</td>\n",
       "      <td>0.469348</td>\n",
       "      <td>0.488619</td>\n",
       "      <td>0.672307</td>\n",
       "      <td>40380.000000</td>\n",
       "      <td>5737.000000</td>\n",
       "      <td>20624.000000</td>\n",
       "      <td>2508.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>75.300000</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>1045.000000</td>\n",
       "      <td>4137.000000</td>\n",
       "      <td>0.490761</td>\n",
       "      <td>0.509239</td>\n",
       "      <td>0.739374</td>\n",
       "      <td>54413.000000</td>\n",
       "      <td>8268.000000</td>\n",
       "      <td>27249.000000</td>\n",
       "      <td>3404.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>80.600000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>1433.750000</td>\n",
       "      <td>5532.750000</td>\n",
       "      <td>0.511381</td>\n",
       "      <td>0.530652</td>\n",
       "      <td>0.783471</td>\n",
       "      <td>74688.000000</td>\n",
       "      <td>11909.000000</td>\n",
       "      <td>36413.000000</td>\n",
       "      <td>4959.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>26.900000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>1953.000000</td>\n",
       "      <td>65528.000000</td>\n",
       "      <td>0.993114</td>\n",
       "      <td>0.962725</td>\n",
       "      <td>0.992776</td>\n",
       "      <td>249750.000000</td>\n",
       "      <td>153365.000000</td>\n",
       "      <td>220253.000000</td>\n",
       "      <td>84414.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>70.900000</td>\n",
       "      <td>72.300000</td>\n",
       "      <td>68.100000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>80.700000</td>\n",
       "      <td>47.400000</td>\n",
       "      <td>22.300000</td>\n",
       "      <td>62.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              State        County      TotalPop           Men         Women  \\\n",
       "count  72718.000000  72718.000000  72718.000000  72718.000000  72718.000000   \n",
       "mean      24.340370    998.356941   4443.485121      0.491322      0.508678   \n",
       "std       15.102552    530.254496   2190.183318      0.040244      0.040244   \n",
       "min        0.000000      0.000000     58.000000      0.037275      0.006886   \n",
       "25%       10.000000    545.000000   2958.000000      0.469348      0.488619   \n",
       "50%       24.000000   1045.000000   4137.000000      0.490761      0.509239   \n",
       "75%       38.000000   1433.750000   5532.750000      0.511381      0.530652   \n",
       "max       51.000000   1953.000000  65528.000000      0.993114      0.962725   \n",
       "\n",
       "       VotingAgeCitizen         Income      IncomeErr   IncomePerCap  \\\n",
       "count      72718.000000   72718.000000   72718.000000   72718.000000   \n",
       "mean           0.717249   61119.999326    9690.325642   30666.653222   \n",
       "std            0.104256   30511.062580    6119.407315   15844.127467   \n",
       "min            0.083826    2692.000000     728.000000    1631.000000   \n",
       "25%            0.672307   40380.000000    5737.000000   20624.000000   \n",
       "50%            0.739374   54413.000000    8268.000000   27249.000000   \n",
       "75%            0.783471   74688.000000   11909.000000   36413.000000   \n",
       "max            0.992776  249750.000000  153365.000000  220253.000000   \n",
       "\n",
       "       IncomePerCapErr  ...       Service        Office  Construction  \\\n",
       "count     72718.000000  ...  72718.000000  72718.000000  72718.000000   \n",
       "mean       4249.725969  ...     18.847948     23.413165      9.263044   \n",
       "std        2991.009809  ...      7.969609      5.591354      5.943849   \n",
       "min         351.000000  ...      0.000000      0.000000      0.000000   \n",
       "25%        2508.000000  ...     13.300000     19.700000      5.000000   \n",
       "50%        3404.000000  ...     17.700000     23.200000      8.400000   \n",
       "75%        4959.000000  ...     23.200000     26.900000     12.500000   \n",
       "max       84414.000000  ...     70.900000     72.300000     68.100000   \n",
       "\n",
       "         Production    WorkAtHome   PrivateWork    PublicWork  SelfEmployed  \\\n",
       "count  72718.000000  72718.000000  72718.000000  72718.000000  72718.000000   \n",
       "mean      12.922312      4.612646     79.511827     14.149495      6.167661   \n",
       "std        7.592511      3.770733      7.957350      7.164790      3.798703   \n",
       "min        0.000000      0.000000     17.500000      0.000000      0.000000   \n",
       "25%        7.200000      2.000000     75.300000      9.300000      3.500000   \n",
       "50%       11.800000      3.800000     80.600000     13.000000      5.500000   \n",
       "75%       17.500000      6.300000     85.000000     17.600000      8.000000   \n",
       "max       60.500000     82.800000    100.000000     80.700000     47.400000   \n",
       "\n",
       "         FamilyWork  Unemployment  \n",
       "count  72718.000000  72718.000000  \n",
       "mean       0.171231      7.224917  \n",
       "std        0.451630      5.099419  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      3.900000  \n",
       "50%        0.000000      6.000000  \n",
       "75%        0.000000      9.000000  \n",
       "max       22.300000     62.800000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Cleaning\n",
    "#Drop Non important columns\n",
    "data.drop(columns=['TractId','Hispanic','White','Black','Native','Asian','Pacific','Employed','MeanCommute','OtherTransp','Walk','Transit','Carpool','Drive'],inplace=True)\n",
    "\n",
    "\n",
    "#Numerical Data into percentages so that it is not skewed by population\n",
    "data['Men'] = data['Men'] / data['TotalPop']\n",
    "data['Women'] = data['Women'] / data['TotalPop']\n",
    "data['VotingAgeCitizen'] = data['VotingAgeCitizen'] / data['TotalPop']\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining the cutoff for our Categories of child poverty\n",
    "##### [.5 points] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. Should balancing of the dataset be done for both the training and testing set? Explain.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We decided to go with pandas built in qcut function which is able to evenly divide any given series into n bins. Since we want 4 types of targets, we gave the function the number 4 for n and it ends up being the cutoff for the quartile ranges of the data. Meaning that the first bin, low child poverty, is the values from the 0th to 25th quartile and the moderate variable is the values from the 25th quartile to the 50th so on and so forth. By doing this, we are left with a roughly equivalent number of entries in each target variable. This is extremely important for the training data because without it, the model could only be good at looking at low poverty rates and would have little data to go off of in the high or extreme categories for example. This idea is not so important for the testing data because when testing, the model should be able to generalize such that it does not need an equivalent number of each class as it is not actively learning from the data and would therefore not be skewed one way or the other. I would argue that it is almost beneficial to have an uneven number of each category in the testing set because that can show if the model is able to generalize well or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChildPoverty\n",
      "low         18229\n",
      "moderate    18171\n",
      "high        18148\n",
      "extreme     18170\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Coming up with divisors for child poverty\n",
    "\n",
    "tmp = pd.qcut(data['ChildPoverty'],4,labels=['low','moderate','high','extreme'])\n",
    "data['ChildPoverty'] = tmp\n",
    "print(data.groupby(['ChildPoverty']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [.5 points] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing. There is NO NEED to split the data multiple times for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data size: 58174 - 80.00% of original data\n",
      "Train_data classes:\n",
      " ChildPoverty\n",
      "low         14596\n",
      "moderate    14447\n",
      "high        14538\n",
      "extreme     14593\n",
      "dtype: int64\n",
      "\n",
      "Test_data size: 14544 - 20.00% of original data\n",
      "Test_data classes:\n",
      " ChildPoverty\n",
      "low         3633\n",
      "moderate    3724\n",
      "high        3610\n",
      "extreme     3577\n",
      "dtype: int64\n",
      "\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Type of y: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    res = res.drop([feature_to_encode], axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Train_data size: {train_data.shape[0]} - {train_data.shape[0] / data.shape[0] * 100:.2f}% of original data')\n",
    "print('Train_data classes:\\n',train_data.groupby(['ChildPoverty']).size()) #Ensure that the number of classes stays relatively equivalent\n",
    "print(f'\\nTest_data size: {test_data.shape[0]} - {test_data.shape[0] / data.shape[0] * 100:.2f}% of original data')\n",
    "print('Test_data classes:\\n',test_data.groupby(['ChildPoverty']).size())\n",
    "\n",
    "y = pd.DataFrame(train_data['ChildPoverty'])\n",
    "X = train_data.drop(columns=['ChildPoverty'],inplace=False)\n",
    "y = encode_and_bind(y,'ChildPoverty')\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Type of X: {type(X)}\\nType of y: {type(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pre-processing and Initial Modeling (2.5 points total)\n",
    "You will be using a two layer perceptron from class for the next few parts of the rubric. There are several versions of the two layer perceptron covered in class, with example code. When selecting an example two layer network from class be sure that you use: (1) vectorized gradient computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization, at a minimum. There is no need to use momentum or learning rate reduction (assuming you choose a sufficiently small learning rate). It is recommended to use sigmoids throughout the network, but not required.\n",
    "[.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs.\n",
    "[.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.\n",
    "[.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.\n",
    "[1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.\n",
    "Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Modeling (5 points total)\n",
    "[1 points] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation). For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm.\n",
    "Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs.\n",
    "[1 points] Repeat the previous step, adding support for a fourth layer.\n",
    "[1 points] Repeat the previous step, adding support for a fifth layer.\n",
    "[2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network (such as AdaGrad, RMSProps, or AdaDelta). Discuss which adaptive method you chose. Compare the performance of your five layer model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique as it is part of the exceptional work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exceptional Work (1 points total)\n",
    "5000 level student: You have free reign to provide additional analyses.\n",
    "One idea (required for 7000 level students):  Implement adaptive momentum (AdaM) in the five layer neural network and quantify the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "86969af4118fd2beaee010b14a97d3c8fe7dc31ff55f1528eea7eaecb45368d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
